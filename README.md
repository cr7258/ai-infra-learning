## AI Infra 学习会议

| 主题 | 时间 | 预习资料 | 录频 | 分享人 
| --- | --- | --- |  ---  | --- |
| vLLM Quickstart | 2025-05-11 | [Doc: vLLM](https://docs.vllm.ai/en/latest/index.html)  | [AI INFRA 学习 01 - LLM 全景图介绍/vLLM 快速入门](https://www.bilibili.com/video/BV1T2EGzLEHi)|  程治玮 |
|PagedAttention| 2025-05-25 | [Blog: vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html)<br>[Paper: Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/pdf/2309.06180)<br>[Video: Fast LLM Serving with vLLM and PagedAttention - 来自 vLLM 作者 Zhuohan Li 的分享](https://www.bilibili.com/video/BV1WUYieQEyL)| | |
| Prefix Caching    |      | [Doc: Automatic Prefix Caching](https://docs.vllm.ai/en/stable/features/automatic_prefix_caching.html) | ||
|Disaggregated Prefilling | | [Doc: Disaggregated Prefilling](https://docs.vllm.ai/en/stable/features/disagg_prefill.html#disaggregated-prefilling-experimental) | | |
| Speculative Decoding |   | [Doc: Speculative Decoding](https://docs.vllm.ai/en/stable/features/spec_decode.html)<br>[Blog: Structured Decoding in vLLM: a gentle introduction](https://blog.vllm.ai/2025/01/14/struct-decode-intro.html)<br>[Paper: Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192) |  | |
| LoRA Adapters    |       | [Doc: LoRA Adapters](https://docs.vllm.ai/en/stable/features/lora.html)<br>[Paper: LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) | ||
| Quantization      |      |       |                      | |
| Distributed Inference and Serving | | [Doc: Distributed Inference and Serving](https://docs.vllm.ai/en/stable/serving/distributed_serving.html)|  ||
