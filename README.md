## AI Infra 学习会议

| 主题 | 时间 | 预习资料 | 录频 | 文档
| --- | --- | --- |  ---  | --- |
| vLLM Quickstart | 2025-05-11 | [Doc: vLLM](https://docs.vllm.ai/en/latest/index.html)  | [AI INFRA 学习 01 - LLM 全景图介绍/vLLM 快速入门](https://www.bilibili.com/video/BV1T2EGzLEHi)|  [01-vllm-quickstart](https://github.com/cr7258/ai-infra-learning/blob/main/lesson/01-vllm-quickstart.md) |
|PagedAttention| 2025-05-25 | [Blog: vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html)<br><br>[Paper: Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/pdf/2309.06180)<br><br>[Video: Fast LLM Serving with vLLM and PagedAttention - 来自 vLLM 作者 Zhuohan Li 的分享](https://www.bilibili.com/video/BV1WUYieQEyL)| [AI INFRA 学习 02 - vLLM PagedAttention 论文精读](https://www.bilibili.com/video/BV1GWjjzfE1b) | [02-pagedattention](https://github.com/cr7258/ai-infra-learning/tree/main/lesson/02-pagedattention/02-pagedattention.md)|
| Prefix Caching    |  2025-06-08    | [Doc: Automatic Prefix Caching](https://docs.vllm.ai/en/stable/features/automatic_prefix_caching.html)<br><br>[Design Doc: Automatic Prefix Caching](https://docs.vllm.ai/en/stable/design/v1/prefix_caching.html)<br><br>[Paper: SGLang: Efficient Execution of Structured Language Model Programs](https://arxiv.org/abs/2312.07104) | | [03-prefix-caching](https://github.com/cr7258/ai-infra-learning/tree/main/lesson/03-prefix-caching/03-prefix-caching.md)|
|Disaggregated Prefilling | | [Doc: Disaggregated Prefilling](https://docs.vllm.ai/en/stable/features/disagg_prefill.html#disaggregated-prefilling-experimental) | | |
| Speculative Decoding |   | [Doc: Speculative Decoding](https://docs.vllm.ai/en/stable/features/spec_decode.html)<br>[Blog: Structured Decoding in vLLM: a gentle introduction](https://blog.vllm.ai/2025/01/14/struct-decode-intro.html)<br>[Paper: Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192) |  | |
| LoRA Adapters    |       | [Doc: LoRA Adapters](https://docs.vllm.ai/en/stable/features/lora.html)<br>[Paper: LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) | ||
| Quantization      |      |       |                      | |
| Distributed Inference and Serving | | [Doc: Distributed Inference and Serving](https://docs.vllm.ai/en/stable/serving/distributed_serving.html)|  ||

## 交流群

<img src=https://github.com/user-attachments/assets/b0451ab2-b16e-4079-8b0a-b5893097572a width=60% />

## 微信公众号

<img src=https://github.com/user-attachments/assets/d2362785-c05a-4b5b-aaa7-49e939ccfc02 width=50% />

![搜索框传播样式-白色版](https://github.com/user-attachments/assets/bf4c1c47-4e85-407b-8143-68a59b474186)
